{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\" media=\"print\">\n",
    "    @media print {\n",
    "      @page { margin: 0; }\n",
    "      body { margin: 1.6cm; }\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Deep Learning - Median House Value Assesment Activity </center></h1>\n",
    "\n",
    "<b><center>April 9th, 2019</center></b>\n",
    " \n",
    "<b><center>Daniel Mínguez Camacho: dani.min.cam@gmail.com   </center><b>\n",
    "<b><center>Javier de la Rúa Martínez: javierdlrm@outlook.com </center><b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/Test.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Submitted to </b> <br>\n",
    "Martin Molina Gonzalez  <br>\n",
    "Daniel Manrique Gamo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"pagebreak\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "This document is created as an assignment for Deep Learning course at Universidad Politécnica de Madrid. The aim of this report is describing the actions performed during the implementation of a deep network using tensorflow. \n",
    "\n",
    "The first section contains a description of how we installed the programming environment and the required libraries. The next section presents the problem and the data used. After that, we describe how we built the network and the results. Finally, we present a short conclusion of our experience during the work performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Problem\n",
    "\n",
    "This California Housing Prices dataset has been downloaded from StatLib repository (http://lib.stat.cmu.edu/datasets/). It is based on data from the 1990 California census, what is not important for deep learning. The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions,” Statistics & Probability Letters 33, no. 3 (1997): 291–297.\n",
    "\n",
    "The problem proposed to solve in this assignment is to predict the house value based on the others variables present in the dataset.\n",
    "\n",
    "## 1.1 Data used\n",
    "\n",
    "We have used the files MedianHouseValuePreparedCleanAttributes.csv and MedianHouseValueOneHotEncodedClasses.csv as described in the assignment description.\n",
    "\n",
    "<b>MedianHouseValuePreparedCleanAttributes.csv</b><br>The original dataset contained 20,640 instances corresponding to districts in california ranging from 600 to 3.000 people. This dataset has been previously cleaned, preprocessed and prepared with the following operations:\n",
    "\n",
    "    - Total_bedrooms attribute has 207 missing values, (na or nan), which are removed since they are very little compared to the whole dataset.\n",
    "    - ISLAND has only 5 samples, not enough to generalize. This class is removed.\n",
    "    - Dataset is randomized.\n",
    "    - Classes are encoded: first discretized and then one-hot encoded.\n",
    "    - Attributes are individually re-scaled, normalized with a min-max scaling within the range [-1,1]: x-(max/2) / (max-min)/2.\n",
    "    - The correlation matrix between all pairs of attributes has been calculated to visualize their dependencies. The results achieved report that total_rooms, total_bedrooms, population and households are highly (positive) correlated.\n",
    "    \n",
    "After this phase of data preparation, a final dataset of 20,433 instances are obtained with 8 attributes (InputsMedianHouseValueNormalized.csv): $longitude$ and $latitude$ (location), $median age$, $total rooms$, $total bedrooms$, $population$, $households$ and $median income$.  \n",
    "\n",
    "From this data, the classification problem consists on estimating the median house value, categorized into the following 10 clases (price intervals in thousand dollards): [15.0, 82.3], [82.4, 107.3], [107.4, 133.9], [134.0, 157.3], [157.4, 179.7], [179.8, 209.4], [209.5, 241.9], [242.0, 290.0], [290.1, 376.6] and [376.7, 500.0]. Each class is labelled from 0 (the cheapest) to 9 (the most expensive), and one-hot encoded in <b>MedianHouseValueOneHotEncodedClasses.csv</b> file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Design process\n",
    "\n",
    "## 2.1 Installation\n",
    "\n",
    "We have decided to use Anaconda as our programming environment. The installation process was as follows:\n",
    "\n",
    "   1. <b> Anaconda download and installation </b>, we downloaded Anaconda from the official webpage (https://www.anaconda.com/distribution/) and installed it following the steps of the program.\n",
    "   2. <b> Environment creation </b> we created a separated environment called tf-gpu for installing the libraries and we added it to jupyter\n",
    "   3. <b> Libraries installation </b>, after that, we installed the following libraries used in the notebook:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} tensorflow-gpu\n",
    "!conda install --yes --prefix {sys.prefix} keras-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4. <b>Test of tensorflow</b>, after the installation we executed the test program provided in class (matrix multiplication) in order to check that tensorflow was working properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Process followed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the development process we focused on using the approach discussed in class:\n",
    "\n",
    "![image.png](Images/Im2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve this, we structured the develoment process in three stages:\n",
    "\n",
    "   1. <b>Models search</b>, in this stage, we executed around 200 models with different architectures and parameters, in order to compare which approaches give better results. We analyzed the outputs using tensorboard.\n",
    "   2. <b> Models choice</b>, we selected the 10 best models according to the test accuracy and we re-executed them with a higher number of epochs. After that we save the models.\n",
    "   3. <b> Ensemble method</b>, using the models obtained in the previous step, we constructed a ensemble network in order to check if it gave a better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Models search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored different combinations of models with different architectures and hyperparameters:\n",
    "\n",
    "   - <b>Activation functions tried</b>: softmax, sigmoid, Hyperbolic tangent, ReLu, ELU, Leaky Relu\n",
    "   - <b>Loss functions tried</b>: Sigmoid cross-entropy, Softmax cross-entropy\n",
    "   - <b>Training techniques tried</b>: Gradient descent, Momentum, RMSprop\n",
    "   - <b>Normalization techniques tried</b> (without taking into account the changes made to the inputs): Dropout, Weights inizialization (Zero, random uniform, random normal, Xavier uniform, Xavier normal)\n",
    "   - <b>DL Architectures tried</b>: (500, 300, 300, 150,75,25,10),(300,150,75,25,10), (100,300,500,400,200,100,50,10), (150,75,25,10),(300,150,75)\n",
    "   - <b>Parameters tried</b>: Learning rate (0.1,0.01,0.001,0.0001), batch size (16-32-100,200) and random\n",
    "   - <b>Metric used</b>: loss and accuracy   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following libraries for the code developed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function used is the following, we input the different paramters to create_run_model, then we create the graph for the network and run the model. The inputs are:\n",
    "   - <b>Inputs</b>: Number of inputs of the network (integer)\n",
    "   - <b>Outputs</b>: Number of outputs of the network (integer)\n",
    "   - <b>Learning rate</b>: for the training algorithm (float)\n",
    "   - <b>n_neurons</b>: Array of integers with the number of neurons per layer (the number of layers is the lenght of this array) (Array)\n",
    "   - <b>batch_norm</b>: boolean, if true batch normalization is applied on each layer\n",
    "   - <b>dropout</b>: boolean, if true batch normalization is applied on each layer\n",
    "   - <b>optimizer</b>: string, type of optimizer to use on each layer. Possible values (\"relu\", \"elu\", \"leakyrelu\", \"softmax\", \"sigmoid\", \"tanh\")\n",
    "   - <b>initb</b>: string, way of initialize the bias, possible values (\"zero\", \"const\"), the const is with value 0.1\n",
    "   - <b>initw</b>: strig, way of initialize the weights, possible values (\"Xavier_Normal\", \"Xavier_Uniform\",\"RUniform\",  \"RNormal\", \"TNormal\"). The Uniform is between (-0.5, 0.5) and the normal has mean 0 and stdev 0.5.\n",
    "   - <b>l2</b>: boleean, if true l2 normalization is applied.\n",
    "   - <b>writter_train</b>: writter for recording the train values for display them in tensorboard\n",
    "   - <b>writter_test</b>: writter for recording the test values for display then in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_model(inputs, outputs, learning_rate, n_neurons, \n",
    "                     batch_norm, activation, loss_fun, \n",
    "                     dropout,optimizer, initb, initw, \n",
    "                     l2, writer_train, writer_test):\n",
    "    \n",
    "    g = create_graph(inputs, outputs, learning_rate, n_neurons,batch_norm,\n",
    "                     activation, loss_fun, dropout, optimizer, initb, initw, l2)\n",
    "    \n",
    "    run_model(writer_train, writer_test, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is created with the function below, the structure is the following:\n",
    "   \n",
    "   1. Create an empty graph\n",
    "   2. Define the inputs to the graph\n",
    "   3. Define the hidden layers and its activations functions\n",
    "   4. Define loss function\n",
    "   5. Define optimizer\n",
    "   6. Define the accuracy\n",
    "   7. Define logs for tensorboard and variables to use in the train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(inputs, outputs, learning_rate, n_neuron,batch_norm, activation, \n",
    "                 loss_fun, dropout, optimizer, initb, initw, l2):\n",
    "    # 1. Create an empty graph\n",
    "\n",
    "    g1 = tf.Graph()\n",
    "    with g1.as_default() as g:\n",
    "        \n",
    "        # 2. Define the inputs to the graph\n",
    "        \n",
    "        training = tf.placeholder_with_default (False, shape=(), name = \"training\") #for batch norm.\n",
    "        X = tf.placeholder (dtype=tf.float32, shape=(None,inputs),name=\"X\")  #Data\n",
    "        t = tf.placeholder (dtype=tf.float32, shape=(None,outputs), name=\"Labels\")  #Labels\n",
    "\n",
    "        # 3. Define the hidden layers and its activations functions\n",
    "        \n",
    "        w = []\n",
    "        hidden_layers = []\n",
    "        layers_temp, w_temp = dense_layer(X,inputs, n_neurons[0], batch_norm, activation, dropout, training, initb, initw)\n",
    "        \n",
    "        hidden_layers.append(layers_temp)\n",
    "        w.append(w_temp)\n",
    "        \n",
    "        for layer in range(1,len(n_neurons)):\n",
    "            layers_temp, w_temp = dense_layer(hidden_layers[layer-1],\n",
    "                                             n_neurons[layer-1],\n",
    "                                             n_neurons[layer],\n",
    "                                             batch_norm, activation, dropout, training, initb, initw)\n",
    "            hidden_layers.append(layers_temp)\n",
    "            w.append(w_temp)\n",
    "\n",
    "        layers_temp, w_temp = dense_layer(hidden_layers[len(n_neurons)-1],\n",
    "                                          n_neurons[-1],\n",
    "                                          outputs,False, \"None\", False, training, initb, initw)\n",
    "        net_out = layers_temp\n",
    "        w.append(w_temp)\n",
    "\n",
    "        y = tf.nn.softmax(logits=net_out, name=\"y\")\n",
    "\n",
    "        # 4. Define loss function\n",
    "        \n",
    "        beta = 0.01\n",
    "        with tf.name_scope(\"Cost\"):\n",
    "            if loss_fun == \"softmax\":\n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2 (labels=t, logits=net_out)\n",
    "            elif loss_fun == \"sigmoid\":\n",
    "                cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=t, logits=net_out)\n",
    "            # Loss function with L2 Regularization with beta=0.01\n",
    "            if l2 == True:\n",
    "                regularizers =tf.nn.l2_loss(w[0])\n",
    "                for i in range(1,len(w)):\n",
    "                    regularizers =  regularizers + tf.nn.l2_loss(w[i])\n",
    "                mean_log_loss = tf.reduce_mean(cross_entropy + beta * regularizers, name=\"mean_log_loss\")    \n",
    "            else:\n",
    "                mean_log_loss = tf.reduce_mean (cross_entropy, name=\"mean_log_loss\")\n",
    "            \n",
    "        # 5. Define optimizer\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            if optimizer == \"Adam\":\n",
    "                train_step = tf.train.AdamOptimizer(learning_rate).minimize(mean_log_loss)\n",
    "            elif optimizer == \"RMSPROP\":\n",
    "                train_step = tf.train.RMSPropOptimizer(learning_rate).minimize(mean_log_loss)\n",
    "            elif optimizer == \"momentum\":\n",
    "                train_step = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(mean_log_loss)\n",
    "            elif optimizer == \"gradient_desc\":\n",
    "                train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_log_loss)\n",
    "\n",
    "        # 6 Define Accuracy\n",
    "        \n",
    "        with tf.name_scope(\"Evaluation\"):\n",
    "            correct_predictions = tf.equal(tf.argmax(y,1),tf.argmax(t,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32))\n",
    "            \n",
    "        # 7 Define logs for tensorboard and variables to use in the train step\n",
    "\n",
    "        tf.summary.scalar(\"cross_entropy\", mean_log_loss)\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        g.add_to_collection(\"elements\", training)\n",
    "        g.add_to_collection(\"elements\", X)\n",
    "        g.add_to_collection(\"elements\", t)\n",
    "        g.add_to_collection(\"elements\", y)\n",
    "        g.add_to_collection(\"elements\", mean_log_loss)\n",
    "        g.add_to_collection(\"elements\", train_step)\n",
    "        g.add_to_collection(\"elements\", correct_predictions)\n",
    "        g.add_to_collection(\"elements\", accuracy)\n",
    "        g.add_to_collection(\"elements\", merged_summary)\n",
    "        \n",
    "    return g1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following function for create each hidden layer cell. It uses the inputs already explained in the previous functions. The structure is the following:\n",
    "\n",
    "   1. Weigts definition\n",
    "   2. Bias definition\n",
    "   3. Activation definition\n",
    "   4. Regularization techiques\n",
    "   5. Logs creation\n",
    "   \n",
    "We use batch normalization and then dropout (both after the activation function after researching about the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(input, channels_in, channels_out, batch_norm, activation, dropout, training, initb, initw, name=\"dense\"):\n",
    "    with tf.name_scope(name):\n",
    "        \n",
    "        # 1. Weights definition \n",
    "        \n",
    "        if initw == \"RUniform\":\n",
    "            w = tf.Variable(tf.random_uniform([channels_in, channels_out], minval=-0.5, maxval = 0.5),name = \"W\")\n",
    "        elif initw == \"Xavier_Normal\":\n",
    "            w = tf.Variable(tf.glorot_normal_initializer()((channels_in, channels_out)),name = \"W\")\n",
    "        elif initw == \"Xavier_Uniform\":\n",
    "            w = tf.Variable(tf.glorot_uniform_initializer()((channels_in, channels_out)),name = \"W\")\n",
    "        elif initw == \"RNormal\":\n",
    "            w = tf.Variable(tf.random_normal([channels_in, channels_out], stddev=0.5),name = \"W\")\n",
    "        elif initw == \"TNormal\":\n",
    "            w = tf.Variable(tf.truncated_normal([channels_in, channels_out], stddev=0.5),name = \"W\")\n",
    "\n",
    "        # 2. Bias definition\n",
    "        \n",
    "        if initb == \"zero\":\n",
    "            b = tf.Variable(tf.constant(0.1, shape = [channels_out]), name = \"b\")\n",
    "        elif initb == \"const\":\n",
    "            b = tf.Variable(tf.zeros(shape = [channels_out]), name = \"b\")\n",
    "            \n",
    "        # 3. Actication definition\n",
    "        \n",
    "        prev_layer = input\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            prev_layer = tf.nn.relu(tf.matmul(prev_layer, w) + b)\n",
    "        elif activation == \"elu\":\n",
    "            prev_layer = tf.nn.elu(tf.matmul(prev_layer, w) + b)\n",
    "        elif activation == \"leakyrelu\":\n",
    "            prev_layer = tf.nn.leaky_relu(tf.matmul(prev_layer, w) + b)\n",
    "        elif activation == \"softmax\":\n",
    "            prev_layer = tf.nn.softmax(tf.matmul(prev_layer, w) + b)\n",
    "        elif activation == \"sigmoid\":\n",
    "            prev_layer = tf.nn.sigmoid(tf.matmul(prev_layer, w) + b)\n",
    "        elif activation == \"tanh\":\n",
    "            prev_layer = tf.nn.tanh(tf.matmul(prev_layer, w) + b)\n",
    "\n",
    "        # 4. Regularization techniques\n",
    "        \n",
    "        if batch_norm:\n",
    "            prev_layer = tf.layers.batch_normalization(prev_layer, training=training) \n",
    "            \n",
    "        if dropout:\n",
    "            prev_layer = tf.nn.dropout(prev_layer, rate = 0.25)\n",
    "        \n",
    "        # 4. Logs creation\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"bias\", b)\n",
    "        tf.summary.histogram(\"act\", prev_layer)\n",
    "        \n",
    "        return prev_layer, w\n",
    "#         act = tf.layers.dense (_input_,output, activation = tf.nn.relu)\n",
    "#         return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run the model for training with the following function:\n",
    "\n",
    "   1. Paramaters definition\n",
    "   2. Graph initialization\n",
    "   3. Retrieve elements used from graph\n",
    "   4. Define the feeds for each call\n",
    "   5. Loop for training\n",
    "   6. Logs for tensorboard\n",
    "   7. Evaluation metrics\n",
    "  \n",
    "We used different values for epochs and batch size for the different models (and fixed batch vs random batch selection also), although here they are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(writer_test, writer_train, g):\n",
    "    \n",
    "    # 1. Parameters definition\n",
    "    \n",
    "    accuracy_train_history = []\n",
    "    n_epochs = 16000\n",
    "    batch_size = 400\n",
    "    \n",
    "    # 2. Graph initialization\n",
    "    \n",
    "    with tf.Session(graph = g) as sess:\n",
    "        \n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer_train.add_graph(sess.graph)\n",
    "        writer_test.add_graph(sess.graph)\n",
    "        \n",
    "        # 3. Retrieve elements used from graph\n",
    "        \n",
    "        training, X, t, y, mean_log_loss, train_step, \\\n",
    "        correct_predictions, accuracy, merged_summary = g.get_collection(\"elements\")\n",
    "\n",
    "        # 4. Define the feeds for each call\n",
    "        \n",
    "        test_feed = {training: False, X: x_test[:NUM_TEST_EXAMPLES],\\\n",
    "                                         t: t_test[:NUM_TEST_EXAMPLES]}\n",
    "        train_feed = {training: False, X: x_train[:NUM_TRAINING_EXAMPLES], \\\n",
    "                                          t: t_train[:NUM_TRAINING_EXAMPLES]}\n",
    "        \n",
    "        # 5. Loop for training\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            offset = np.random.randint(0,NUM_TRAINING_EXAMPLES,300)\n",
    "\n",
    "            feed = {training: True, X: x_train[offset], t: t_train[offset]}\n",
    "            feed_eval = {training: False, X: x_train[offset], t: t_train[offset]}\n",
    "\n",
    "            # 6. Logs for tensorboard\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                s_train = sess.run(merged_summary, feed_dict=feed_eval)\n",
    "                s_test = sess.run(merged_summary, feed_dict=test_feed)\n",
    "                writer_train.add_summary(s_train,epoch)\n",
    "                writer_test.add_summary(s_test,epoch)\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict=feed_eval)\n",
    "                test_accuracy = sess.run(accuracy, feed_dict=test_feed)\n",
    "                print(\"step %d, training accuracy %g, test accuracy %g\" % (epoch, train_accuracy, test_accuracy))     \n",
    "\n",
    "            sess.run (train_step, feed_dict=feed)\n",
    "        \n",
    "        # 7. Evaluation metrics\n",
    "        \n",
    "        accuracy_test = accuracy.eval(feed_dict=test_feed)\n",
    "        accuracy_train = accuracy.eval(feed_dict=train_feed)\n",
    "\n",
    "        test_predictions = y.eval(feed_dict={X: x_test[:NUM_TEST_EXAMPLES]})\n",
    "        test_correct_preditions = correct_predictions.eval (feed_dict=test_feed)\n",
    "\n",
    "        train_mean_log_loss = mean_log_loss.eval(feed_dict = test_feed)\n",
    "        test_mean_log_loss = mean_log_loss.eval(feed_dict = test_feed)\n",
    "        \n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these function now we launch our search, first we read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (16346, 8)\n",
      "t_train: (16346, 10)\n",
      "x_dev: (2043, 8)\n",
      "t_dev: (2043, 10)\n",
      "x_test: (2044, 8)\n",
      "t_test: (2044, 10)\n"
     ]
    }
   ],
   "source": [
    "run 1.ReadingData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = x_train.shape[1]\n",
    "OUTPUTS = t_train.shape[1]\n",
    "NUM_TRAINING_EXAMPLES = int(round(x_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int (round (x_dev.shape[0]/1))\n",
    "NUM_TEST_EXAMPLES = int (round (x_test.shape[0]/1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define one more function for creating an string for naming the folders created for tensorflow (and so be able to query them using regex expressions), we basically input all the parameters used and concatenate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hparam_string(initb, initw, learning_rate, n_neurons, batch_norm, optimizer, activation, loss_fun, dropout, l2):\n",
    "    output = \"learning_rate = \" + str(learning_rate) + \", neurons = \" + str(n_neurons) + \\\n",
    "            \", batch_norm =\" + str(batch_norm) + \", opt = \" + str(optimizer) + \", act = \" + str(activation) + \\\n",
    "            \", loss = \" + str(loss_fun) + \", drop = \" + str(dropout) + \", binit = \" + str(initb) + \\\n",
    "            \", winit = \" + str(initw) + \", l2 = \" + str(l2)\n",
    "    \n",
    "    return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to define the parameters, we include all the parameters we tried below, although we didn´t tried all the combinations for computing time reasons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = [\"Adam\", \"RMSPROP\", \"momentum\", \"gradient_desc\"]\n",
    "act = [\"relu\", \"elu\", \"leakyrelu\", \"softmax\", \"sigmoid\", \"tanh\"]\n",
    "loss = [\"softmax\", \"sigmoid\"]\n",
    "BInit = [\"zero\", \"const\"]\n",
    "WInit = [\"Xavier_Normal\", \"Xavier_Uniform\",\"RUniform\",  \"RNormal\", \"TNormal\"]\n",
    "batch_norm = [True, False]\n",
    "dropout = [True, False]\n",
    "l2 = [True, False]\n",
    "rates = [1E-1, 1E-2, 1E-3, 1E-4]\n",
    "neurons = [[500, 300, 300, 150,75,25,10],[300,150,75,25,10], \\\n",
    "           [100,300,500,400,200,100,50,10], [150,75,25,10],[300,150,75]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run the different models (again although its displayed like these, we selected different combinations of the parameters above for the different runs we did), basically we:\n",
    "\n",
    "   1. Initialize the loop\n",
    "   2. Create the folder string\n",
    "   3. Print the model for log purposes\n",
    "   4. Create the tensorboard writters\n",
    "   5. Create and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "MODEL learning_rate = 0.001, neurons = [500, 300, 300, 150, 75, 25, 10], batch_norm =False, opt = Adam, act = elu, loss = softmax, drop = False, binit = const, winit = Xavier_Uniform, l2 = True\n",
      "----------------------------\n",
      "step 0, training accuracy 0.156667, test accuracy 0.108121\n",
      "step 500, training accuracy 0.286667, test accuracy 0.326321\n",
      "step 1000, training accuracy 0.303333, test accuracy 0.325342\n",
      "step 1500, training accuracy 0.34, test accuracy 0.328767\n",
      "step 2000, training accuracy 0.333333, test accuracy 0.322896\n",
      "step 2500, training accuracy 0.326667, test accuracy 0.324364\n",
      "step 3000, training accuracy 0.31, test accuracy 0.311155\n",
      "step 3500, training accuracy 0.316667, test accuracy 0.322407\n",
      "step 4000, training accuracy 0.306667, test accuracy 0.323386\n",
      "step 4500, training accuracy 0.353333, test accuracy 0.323386\n",
      "step 5000, training accuracy 0.323333, test accuracy 0.323386\n",
      "step 5500, training accuracy 0.35, test accuracy 0.326321\n",
      "step 6000, training accuracy 0.326667, test accuracy 0.322407\n",
      "step 6500, training accuracy 0.336667, test accuracy 0.318493\n",
      "step 7000, training accuracy 0.32, test accuracy 0.330235\n",
      "step 7500, training accuracy 0.383333, test accuracy 0.325832\n",
      "step 8000, training accuracy 0.3, test accuracy 0.328278\n",
      "step 8500, training accuracy 0.326667, test accuracy 0.322896\n",
      "step 9000, training accuracy 0.336667, test accuracy 0.323875\n",
      "step 9500, training accuracy 0.32, test accuracy 0.330724\n",
      "step 10000, training accuracy 0.333333, test accuracy 0.324364\n",
      "step 10500, training accuracy 0.346667, test accuracy 0.316536\n",
      "step 11000, training accuracy 0.266667, test accuracy 0.324364\n",
      "step 11500, training accuracy 0.333333, test accuracy 0.317025\n",
      "step 12000, training accuracy 0.296667, test accuracy 0.321918\n",
      "step 12500, training accuracy 0.3, test accuracy 0.319472\n",
      "step 13000, training accuracy 0.313333, test accuracy 0.325832\n",
      "step 13500, training accuracy 0.31, test accuracy 0.323875\n",
      "step 14000, training accuracy 0.266667, test accuracy 0.325342\n",
      "step 14500, training accuracy 0.336667, test accuracy 0.321918\n",
      "step 15000, training accuracy 0.306667, test accuracy 0.321918\n",
      "step 15500, training accuracy 0.33, test accuracy 0.319472\n"
     ]
    }
   ],
   "source": [
    "# This variable is for ordering the folders that are created\n",
    "i = 0\n",
    "\n",
    "# 1. Initialize the loop\n",
    "\n",
    "for optimizer in [\"Adam\"]:\n",
    "    for activation in [\"elu\"]:\n",
    "        for loss_fun in [\"softmax\"]:\n",
    "            for initb in BInit:\n",
    "                for initw in WInit:\n",
    "                    for batch_norm in [False]:\n",
    "                        for dropout in [False]:\n",
    "                            for learning_rate in rates:\n",
    "                                for n_neurons in neurons:\n",
    "                                    \n",
    "                                    # 2. Create the folder string\n",
    "                                    \n",
    "                                    hparam_str = make_hparam_string(initb, initw, \\\n",
    "                                                                    learning_rate,\\\n",
    "                                                                    n_neurons, batch_norm, \\\n",
    "                                                                    optimizer, activation, \\\n",
    "                                                                    loss_fun, dropout, l2)\n",
    "                                    i = i + 1\n",
    "                                    \n",
    "                                    # 3. Print the model for log purposes\n",
    "                                    \n",
    "                                    print(\"----------------------------\")\n",
    "                                    print(\"MODEL \" + hparam_str)\n",
    "                                    print(\"----------------------------\")      \n",
    "                                    \n",
    "                                    # 4. Create the tensorboard writters\n",
    "                                    \n",
    "                                    writer_train = tf.summary.FileWriter(\"/TF_Logs/20190604_2/\"\\\n",
    "                                                                         + str(i) + \" Train \" + \\\n",
    "                                                                         hparam_str)\n",
    "                                    writer_test = tf.summary.FileWriter(\"/TF_Logs/20190604_2/\" \\\n",
    "                                                                        + str(i) + \" Test \" + \\\n",
    "                                                                        hparam_str)\n",
    "\n",
    "                                    # 5. Create and run the model\n",
    "\n",
    "                                    create_run_model(INPUTS,OUTPUTS, learning_rate,n_neurons, \\\n",
    "                                                     batch_norm,activation,loss_fun,dropout,optimizer, \\\n",
    "                                                     initb, initw, l2, writer_test,writer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy followed during the search was executing a large number of models with low epochs and non-random batch size at the beggining in order to check if the model improve and how quickly. After that we selected the parameters that worked better for a second execution with less models but more epochs. Finally we executed a small number of models with more epochs and specific parameters.\n",
    "\n",
    "The cmd code used to execute tensorboard is (remember that the logs are stored in the path were jupyter is executed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir /logs/1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we stated in the first execution we obtained arond 120 models:\n",
    "\n",
    "![image.png](Images/Im12.png)\n",
    "\n",
    "We analyzed the results with tensorflow, here we can see the best 11 models with 2000 epochs:\n",
    "\n",
    "![image.png](Images/Im3.png)\n",
    "\n",
    "The best results were obtained with ********pending*********\n",
    "\n",
    "Using tensorboard we can also see how the weights evolve over time and the difference between different inizialization methods. For example, below we can see how the bias of two different models evolve over time (initialized in 0.1) and the difference in the weights of the last layer and the first (initialized with random normal), we can see how the last layer is more updated and the first layer remains normal (due to the low epochs applied):\n",
    "\n",
    "![image.png](Images/Im13.png)\n",
    "\n",
    "We can also see the graphs structures of the different models (left example with batch normalization and right without):\n",
    "\n",
    "![image.png](Images/Im14.png)\n",
    "\n",
    "And also we can zoom to see inside (for example the dropout):\n",
    "\n",
    "![image.png](Images/Im7.png)\n",
    "\n",
    "In summary, after that we executed again using 4000 epochs ***pendinng****\n",
    "\n",
    "![image.png](Images/Im15.png)\n",
    "\n",
    "And finally we executed some models for a much large number of epochs, obtaining the following test resutls ***pending***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Models Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we selected the 5 models that gave us better results and we reexecuted them again, saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built an ensemble method with the models we selected before by averaging the results of each one in order to see if the results improve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end we obtained the following results for the 5 models and the enseble: ***Pending***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-9ed954051357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m\"Accuracy for the TRAIN set: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_train' is not defined"
     ]
    }
   ],
   "source": [
    "\"Accuracy for the TRAIN set: \" + str(accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy for the TEST set: 0.44520548'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Accuracy for the TEST set: \" + str(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1242537e-03, 3.5785377e-02, 1.7773926e-01, ..., 4.8553996e-02,\n",
       "        2.5984354e-02, 2.4259519e-03],\n",
       "       [4.2458699e-04, 1.6331108e-03, 1.2209060e-02, ..., 1.3920738e-01,\n",
       "        3.5770394e-02, 6.7899786e-03],\n",
       "       [9.4441921e-01, 5.3289428e-02, 1.6226717e-03, ..., 1.0505806e-04,\n",
       "        1.6822800e-04, 2.7108581e-05],\n",
       "       ...,\n",
       "       [2.5135069e-04, 4.6813592e-02, 2.8784597e-01, ..., 9.2178322e-03,\n",
       "        2.9628810e-03, 2.6307136e-04],\n",
       "       [1.3063335e-06, 3.7781382e-03, 7.8285262e-02, ..., 1.0381859e-02,\n",
       "        3.4393470e-03, 1.2517342e-03],\n",
       "       [7.9505953e-06, 2.6903521e-05, 2.4926532e-03, ..., 8.9958467e-02,\n",
       "        2.7572110e-02, 9.0186866e-03]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rounded_predictions=np.round(test_predictions)\n",
    "indices = np.argmax(test_predictions,1)\n",
    "for row, index in zip(test_rounded_predictions, indices): row[index]=1\n",
    "test_rounded_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test[:10] #target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False,  True,  True, False,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_correct_preditions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Conclusions\n",
    "\n",
    "Here the conclusions BLA BLA BLA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
